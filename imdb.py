# -*- coding: utf-8 -*-
"""IMDb.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JjZHymyul1daiQuO1z4t15UZvHkJfphs
"""

import sys
import sklearn
import tensorflow as tf
from tensorflow import keras
import numpy as np
import os

# Commented out IPython magic to ensure Python compatibility.
# Python ≥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Is this notebook running on Colab or Kaggle?
IS_COLAB = "google.colab" in sys.modules
IS_KAGGLE = "kaggle_secrets" in sys.modules

if IS_COLAB:
#    %pip install -q -U tensorflow-addons
#     %pip install -q -U transformers

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

# TensorFlow ≥2.0 is required
import tensorflow as tf
from tensorflow import keras
assert tf.__version__ >= "2.0"

if not tf.config.list_physical_devices('GPU'):
    print("No GPU was detected. LSTMs and CNNs can be very slow without a GPU.")
    if IS_COLAB:
        print("Go to Runtime > Change runtime and select a GPU hardware accelerator.")
    if IS_KAGGLE:
        print("Go to Settings > Accelerator and select GPU.")

# to make this notebook's output stable across runs
np.random.seed(42)
tf.random.set_seed(42)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "nlp"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)
def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

tf.random.set_seed(42)# to make this notebook's output stable across runs

(X_train, y_train), (X_test, y_test) = keras.datasets.imdb.load_data()
#加载IMDb评论数据集，包含50000条英语电影评论，25000用于测试，25000用于训练。
#每条评论有一个标签，负面为0,正面为1.

#展示
X_train[0][:15]
#X_train中每个评论由一个np整数数组表示，每个整数表示一个单词
#这里取训练集中第一条评论的前15个单词显示

#可视化评论
word_index = keras.datasets.imdb.get_word_index()#获取评论的标签
id_to_word = {id_ + 3: word for word, id_ in word_index.items()}
#给评论中每一个单词分配一个数字id，从第3个开始分配，前三个为"<pad>"（填充符）, "<sos>"（序列开始）, "<unk>"（未知）

#演示
for id_, token in enumerate(("<pad>", "<sos>", "<unk>")):
    id_to_word[id_] = token#给每个token（单词）分配id
" ".join([id_to_word[id_] for id_ in X_train[0][:10]])#每个单词用空格分割

import tensorflow_datasets as tfds
#以文本（字节字符串）的形式加载原始的IMDb评论
datasets, info = tfds.load("imdb_reviews", as_supervised=True, with_info=True)

datasets.keys()#打印数据集的类型

train_size = info.splits["train"].num_examples
test_size = info.splits["test"].num_examples

train_size, test_size

#演示
for X_batch, y_batch in datasets["train"].batch(2).take(1):#取出训练集中的2条评论，逐一进行以下循环，
                                #其中X_batch为评论的文本数据, y_batch为评论的标签
    for review, label in zip(X_batch.numpy(), y_batch.numpy()):
        print("Review:", review.decode("utf-8")[:200], "...")#只输出处理的评论的前200个单词
        print("Label:", label, "= Positive" if label else "= Negative")#输出处理的评论的类型
        print()

#定义预处理函数
def preprocess(X_batch, y_batch):
    X_batch = tf.strings.substr(X_batch, 0, 300)#从截断评论开始，每条评论保留前300个字符
    X_batch = tf.strings.regex_replace(X_batch, rb"<br\s*/?>", b" ")#用空格替换<br/>
    X_batch = tf.strings.regex_replace(X_batch, b"[^a-zA-Z']", b" ")#用空格替换字母和引号以外的所有字符
    X_batch = tf.strings.split(X_batch)#将评论按照空格分割，返回代表代表每条评论转化成的不规则长度的向量
    return X_batch.to_tensor(default_value=b"<pad>"), y_batch#将所有评论用<pad>填充使得所有向量长度相同

#演示
preprocess(X_batch, y_batch)#预处理并打印取出的两条评论

from collections import Counter#使用Counter对单词出现的次数计数

vocabulary = Counter()#构建词汇表
for X_batch, y_batch in datasets["train"].batch(32).map(preprocess):#取32条训练集中的评论批量预处理
    for review in X_batch:
        vocabulary.update(list(review.numpy()))#更新词汇表

#演示
vocabulary.most_common()[:3]#三个最常见的词

len(vocabulary)

vocab_size = 10000#截断词表，减小词表长度至10000，保留出现频率前10000的词语
truncated_vocabulary = [
    word for word, count in vocabulary.most_common()[:vocab_size]]

word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}#将每个单词替换成该单词对应的id
#演示
for word in b"This movie was faaaaaantastic".split():
    print(word_to_id.get(word) or vocab_size)#在前10000个单词中未出现的词语用10000表示

#创建查找表
words = tf.constant(truncated_vocabulary)#用前10000个词语定义词汇表
word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)#创建相关索引
vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)#为查找表创建初始化程序，将类别列表及其对应索引传递给它
num_oov_buckets = 1000 #1000个存储桶
table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)
#当查找词表中不存在的类别时查找表会计算该类别的哈希值，将这个未知类别分配给一个oov桶，它的类别从已知类别开始

#演示用查找表查找几个单词的id
table.lookup(tf.constant([b"This movie was faaaaaantastic".split()]))
#基于数据样本定义词汇表，为不在数据样本中的其他类别添加一些桶
#faaaaaantastic映射到ID＞=10000的一个oov桶中了

#创建最终的训练集
def encode_words(X_batch, y_batch):#对单词进行编码
    return table.lookup(X_batch), y_batch


train_set = datasets["train"].batch(32).map(preprocess)#将32条评论转化为一个单词的短序列的[ [] [] ]形式的Tensor
train_set = train_set.map(encode_words).prefetch(1)#将单词短序列转换成单词id短序列

test_set = datasets["test"].batch(32).map(preprocess)#将32条评论转化为一个单词的短序列的[ [] [] ]形式的Tensor
test_set = test_set.map(encode_words).prefetch(1)#将单词短序列转换成单词id短序列

#演示
for X_batch, y_batch in train_set.take(1):#从训练集中取出一条评论
    print(X_batch)
    print(y_batch)

#创建模型，开始训练
embed_size = 128  #步长
#嵌入维度为128维
model = keras.models.Sequential([
                                 #嵌入矩阵形状为（10000+1000,128）即（11000,128），每一行为一个tensor
                                 #一个tensor包含32条评论，每条评论中显示前300个单词，一次处理128个Tensor（128列）
                                 #将所有索引标号映射到致密的低维向量：[[4],[32],[67]]被映射为[[0.3,0.9,0.2]
    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,#随机初始化嵌入矩阵，使用索引调用时返回嵌入矩阵中该索引在的行
                           mask_zero=True, 
                           input_shape=[None]),
    keras.layers.GRU(128, return_sequences=True),#使用128个神经元的GRU层（未指定输入的形状）。预测评论的类型
    #由于GRU堆叠使用，故添加return_sequences=True使得下一个GRU层有三维输入
    keras.layers.GRU(128),#返回最后一个时间步长的输出
    keras.layers.Dense(1, activation="sigmoid")#使用sigmoid输出估计为Positive类型概率的单个神经元
])
#Embedding层将单词id转换为嵌入，训练期间嵌入会逐渐改善，梯度下降会使得该评论渐渐靠近其所属类别
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])#计算模型的二元交叉熵作为loss，使用adam优化，计算每一层的ACC
history = model.fit(train_set, epochs=15)#迭代次数，训练的轮数为15
#782*batch_size=25000,782=25000/32，是每轮次处理的tensor数量

test = model.fit(test_set, epochs=15)#迭代次数，训练的轮数为15
#782*batch_size=25000,782=25000/32，是每轮次处理的tensor数量











